{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQNf6YfTbrLeDIl8YNG5D9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0xDebabrata/tinygrad-nano-gpt/blob/main/tinygrad_nano_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EggF74pv891N"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install tinygrad"
      ],
      "metadata": {
        "id": "q44IKuSirpVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "CLxudeDGjZg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "\n",
        "Training data is converted into tokens which is how the model 'sees' any data.\n",
        "Tokenization is a play between the vocabulary size and the sequence length.\n",
        "\n",
        "We only consider every character as a separate token, and assign it some integer value. More complicated tokenizers exist like [tiktoken](https://github.com/openai/tiktoken) and [sentencepiece](https://github.com/google/sentencepiece) which are sub-word level encoders. Naturally, the vocabulary size increases but sequence length decreases."
      ],
      "metadata": {
        "id": "dbJCBgU6k-rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGA55trZlBM-",
        "outputId": "e1b171de-d324-41fe-f108-662e1f6059b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = { ch: i for i, ch in enumerate(chars) }\n",
        "itos = { i: ch for i, ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]             # take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l])    # take a list of integers, output a string\n",
        "\n",
        "print(encode(\"Hello world\"))\n",
        "print(decode(encode(\"Hello world\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI5aSIREmS4B",
        "outputId": "a6e7a5c7-a50d-48c0-df1a-9d64aaeff011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
            "Hello world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We tokenize the entire dataset and store it as a tensor."
      ],
      "metadata": {
        "id": "hClTFfwht5r9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tinygrad.tensor import Tensor\n",
        "from tinygrad.helpers import dtypes\n",
        "\n",
        "data = Tensor(encode(text), dtype=dtypes.int32)\n",
        "print(len(data.numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5-8bzf2q8Vj",
        "outputId": "0a59b3e2-c47d-4c32-d512-663df5e9bf9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9 * len(data.numpy()))\n",
        "training_data = data[:n]\n",
        "validation_data = data[n:]"
      ],
      "metadata": {
        "id": "Pwy4ju5gtLTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do not train the model on the entire dataset at once. That is computationally very expensive. We take random chunks of a certain size called the `context length`."
      ],
      "metadata": {
        "id": "3hHfdqoMuHWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8          # Also known as context length\n",
        "training_data[:block_size + 1].numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkKH8TOvuB_j",
        "outputId": "bb5f645b-cd6d-4883-ee11-dd410c65174e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([18, 47, 56, 57, 58,  1, 15, 47, 58], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How the transformer is trained\n",
        "\n",
        "When the transformer network sees this context of data, it tries to map several things. The primary goal is predict the next token, given an existing set of tokens.\n",
        "\n",
        "From the output of the above code block (the first context set):\n",
        "- [18] is used to predict 47.\n",
        "- [18, 47] is used to predict 56, and so on.\n",
        "\n",
        "So the context actually contains `block_size + 1` tokens, but it infers `block_size` number of results.\n",
        "\n",
        "While this chunking helps make it computationally less intensive, it also allows the network to predict the next token from a context of size `1` up to `block_size`"
      ],
      "metadata": {
        "id": "FPDasGp7vjF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spelled out in code\n",
        "x = training_data[:block_size + 1]\n",
        "y = training_data[1 : block_size + 1]\n",
        "for i in range(block_size):\n",
        "    context = x[:i + 1]\n",
        "    target = y[i]\n",
        "    print(f\"For context {context.numpy()}, the target is {target.numpy()}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR2hYDN2wjE1",
        "outputId": "25e24c70-d318-40c6-8a7d-ef528757126c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For context [18], the target is 47.\n",
            "For context [18 47], the target is 56.\n",
            "For context [18 47 56], the target is 57.\n",
            "For context [18 47 56 57], the target is 58.\n",
            "For context [18 47 56 57 58], the target is 1.\n",
            "For context [18 47 56 57 58  1], the target is 15.\n",
            "For context [18 47 56 57 58  1 15], the target is 47.\n",
            "For context [18 47 56 57 58  1 15 47], the target is 58.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is further improved by processing each context parallely in batches. This helps improve GPU utilisation."
      ],
      "metadata": {
        "id": "tSyo0Xfc0RTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "block_size = 8\n",
        "Tensor.manual_seed = 69\n",
        "\n",
        "def get_batch(split):\n",
        "    data = training_data if split == 'train' else validation_data\n",
        "    # Generate random indices for each chunk\n",
        "    ix = Tensor.uniform(batch_size, low=0, high=len(data.numpy()) - block_size, dtype=dtypes.int32)\n",
        "    contexts = Tensor.stack([data[idx:idx+block_size] for idx in ix.numpy()])\n",
        "    targets = Tensor.stack([data[idx+1:idx+block_size+1] for idx in ix.numpy()])\n",
        "    return contexts, targets\n",
        "\n",
        "# xb -> batch of inputs\n",
        "# yb -> batch of targets\n",
        "xb, yb = get_batch('train')\n",
        "print(\"Inputs:\")\n",
        "print(xb.numpy())\n",
        "print(\"Targets:\")\n",
        "print(yb.numpy())\n",
        "\n",
        "\n",
        "# # Spelled out in code\n",
        "# for i in range(batch_size):\n",
        "#     for j in range(block_size):\n",
        "#         context = xb[i][:j + 1]\n",
        "#         target = yb[i][j]\n",
        "#         print(f\"For context {context.numpy()}, the target is {target.numpy()}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmZwEiPb0bEo",
        "outputId": "01e6db38-74ef-44be-b53d-6adc03aaf510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            "[[56 47 43 52 42  1 53 56]\n",
            " [ 6  1 21  1 61 47 50 50]\n",
            " [50 54  1 46 47 51  6  1]\n",
            " [43 56  0 25 63 57 43 50]]\n",
            "Targets:\n",
            "[[47 43 52 42  1 53 56  1]\n",
            " [ 1 21  1 61 47 50 50  1]\n",
            " [54  1 46 47 51  6  1 63]\n",
            " [56  0 25 63 57 43 50 44]]\n"
          ]
        }
      ]
    }
  ]
}